{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the target variable distribution\n",
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFYCAYAAADOev/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHDFJREFUeJzt3W9slfX9//HXdXp6rNVTy6k9bjDmVtHCTFvEKlCsWLQ6\niWHoSoVSl31XN52FQKxi1yiUYC0Wu4HCIn+GNiDS2N2prmmZrDSyHrrhWbC4ILCYhRTXnrO1Qlvw\n1Pb8buzniRWFg+vpkX6ej8Rk59PrfPq+vOGeua5zrlrBYDAoAABgLFu0BwAAANFFDAAAYDhiAAAA\nwxEDAAAYjhgAAMBwxAAAAIazR3uAaPH5Tkd7BAAARk1ysvMrf8aVAQAADEcMAABgOGIAAADDEQMA\nABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBwxv7VwkhZ\ntq4+2iMAI2LDk/OiPQKAUcKVAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEA\nAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBwxAAAAIYjBgAAMBwxAACA4YgBAAAMRwwAAGA4\nYgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAA\nwHDEAAAAhrNHcvOqqiq9++67+vTTT/XII4/oT3/6k95//30lJiZKkoqKinTHHXeovr5eNTU1stls\nys/P14IFCzQwMKDS0lKdPHlSMTExqqys1MSJE3XkyBGVl5dLklJTU7V69WpJ0rZt29TY2CjLsrRk\nyRLNnj07kqcGAMCYEbEYOHDggI4dO6ba2lp1d3fr/vvv14wZM/T4448rJycndFx/f782bdqkuro6\nxcbGKi8vT7m5uWpublZCQoKqq6u1f/9+VVdXa/369aqoqFBZWZnS09NVUlKilpYWpaSkqKGhQbt3\n71Zvb68KCgp02223KSYmJlKnBwDAmBGx2wS33HKLNmzYIElKSEjQmTNnNDg4eM5xhw4dUlpampxO\np+Li4jRt2jR5vV55PB7l5uZKkrKysuT1ehUIBNTR0aH09HRJUk5Ojjwej9ra2pSdnS2HwyGXy6UJ\nEybo+PHjkTo1AADGlIhdGYiJiVF8fLwkqa6uTrfffrtiYmK0c+dOvfLKK0pKStIzzzwjv98vl8sV\nep/L5ZLP5xu2brPZZFmW/H6/EhISQscmJSXJ5/MpMTHxS/dITU39yvnGjYuX3c6VA+CrJCc7oz0C\ngFES0c8MSNLbb7+turo6bd++XYcPH1ZiYqKmTJmiLVu2aOPGjbrpppuGHR8MBr90ny9bv5hjv6i7\nuz+M6QFz+Xynoz0CgBF0vsCP6LcJ3nnnHb388svaunWrnE6nZs6cqSlTpkiS5syZo6NHj8rtdsvv\n94fe09XVJbfbLbfbLZ/PJ0kaGBhQMBhUcnKyenp6Qsd2dnaGjv38Hp+tAwCAC4tYDJw+fVpVVVXa\nvHlz6NsDS5cu1YkTJyRJbW1tuv7665WRkaH29nadOnVKfX198nq9yszM1KxZs9TY2ChJam5u1vTp\n0xUbG6uUlBQdPHhQkrRnzx5lZ2drxowZ2rdvnwKBgDo7O9XV1aVJkyZF6tQAABhTInaboKGhQd3d\n3Vq+fHlo7YEHHtDy5ct1+eWXKz4+XpWVlYqLi1NJSYmKiopkWZaKi4vldDo1d+5ctba2atGiRXI4\nHFq7dq0kqaysTCtXrtTQ0JAyMjKUlZUlScrPz1dhYaEsy1J5eblsNh6hAABAOKxgODfYx6BI3Q9d\ntq4+IvsCo23Dk/OiPQKAERS1zwwAAIBvPmIAAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDh\niAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBwxAAAAIYjBgAAMBwxAACA4YgBAAAMRwwAAGA4YgAA\nAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHDE\nAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBwxAAAAIYjBgAAMBwxAACA\n4YgBAAAMRwwAAGA4eyQ3r6qq0rvvvqtPP/1UjzzyiNLS0rRixQoNDg4qOTlZ69atk8PhUH19vWpq\namSz2ZSfn68FCxZoYGBApaWlOnnypGJiYlRZWamJEyfqyJEjKi8vlySlpqZq9erVkqRt27apsbFR\nlmVpyZIlmj17diRPDQCAMSNiMXDgwAEdO3ZMtbW16u7u1v3336+ZM2eqoKBA9957r37961+rrq5O\n8+fP16ZNm1RXV6fY2Fjl5eUpNzdXzc3NSkhIUHV1tfbv36/q6mqtX79eFRUVKisrU3p6ukpKStTS\n0qKUlBQ1NDRo9+7d6u3tVUFBgW677TbFxMRE6vQAABgzInab4JZbbtGGDRskSQkJCTpz5oza2tp0\n5513SpJycnLk8Xh06NAhpaWlyel0Ki4uTtOmTZPX65XH41Fubq4kKSsrS16vV4FAQB0dHUpPTx+2\nR1tbm7Kzs+VwOORyuTRhwgQdP348UqcGAMCYErErAzExMYqPj5ck1dXV6fbbb9f+/fvlcDgkSUlJ\nSfL5fPL7/XK5XKH3uVyuc9ZtNpssy5Lf71dCQkLo2M/2SExM/NI9UlNTv3K+cePiZbdz5QD4KsnJ\nzmiPAGCURPQzA5L09ttvq66uTtu3b9fdd98dWg8Gg196/MWsX+wen9fd3X/BYwCT+Xynoz0CgBF0\nvsCP6LcJ3nnnHb388svaunWrnE6n4uPjdfbsWUlSZ2en3G633G63/H5/6D1dXV2hdZ/PJ0kaGBhQ\nMBhUcnKyenp6Qsd+1R6frQMAgAuLWAycPn1aVVVV2rx5sxITEyX9995/U1OTJGnPnj3Kzs5WRkaG\n2tvbderUKfX19cnr9SozM1OzZs1SY2OjJKm5uVnTp09XbGysUlJSdPDgwWF7zJgxQ/v27VMgEFBn\nZ6e6uro0adKkSJ0aAABjSsRuEzQ0NKi7u1vLly8Pra1du1ZPP/20amtrNX78eM2fP1+xsbEqKSlR\nUVGRLMtScXGxnE6n5s6dq9bWVi1atEgOh0Nr166VJJWVlWnlypUaGhpSRkaGsrKyJEn5+fkqLCyU\nZVkqLy+XzcYjFAAACIcVDOcG+xgUqfuhy9bVR2RfYLRteHJetEcAMIKi9pkBAADwzUcMAABgOGIA\nAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBw\nxAAAAIYjBgAAMBwxAACA4YgBAAAMRwwAAGA4YgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAA\ngOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYLiw\nYqC0tPSctaKiohEfBgAAjD77+X5YX1+v3bt369ixY1q8eHFofWBgQH6/P+LDAQCAyDtvDMybN0/T\np0/XE088oaVLl4bWbTabJk2aFPHhAABA5J03BiTpmmuu0Y4dO3T69Gn19PSE1k+fPq3ExMSIDgcA\nACLvgjEgSc8++6x+//vfy+VyKRgMSpIsy9LevXsjOhwAAIi8sGKgra1NBw4c0GWXXRbpeQAAwCgL\n69sE1157LSEAAMAYFdaVgW9961tavHixbr75ZsXExITWly1bdt73HT16VI899ph++tOfqrCwUKWl\npXr//fdDnzUoKirSHXfcofr6etXU1Mhmsyk/P18LFizQwMCASktLdfLkScXExKiyslITJ07UkSNH\nVF5eLklKTU3V6tWrJUnbtm1TY2OjLMvSkiVLNHv27K/z7wMAAOOEFQOJiYmaOXPmRW3c39+vNWvW\nnPO+xx9/XDk5OcOO27Rpk+rq6hQbG6u8vDzl5uaqublZCQkJqq6u1v79+1VdXa3169eroqJCZWVl\nSk9PV0lJiVpaWpSSkqKGhgbt3r1bvb29Kigo0G233TYsXAAAwJcLKwYee+yxi97Y4XBo69at2rp1\n63mPO3TokNLS0uR0OiVJ06ZNk9frlcfj0fz58yVJWVlZKisrUyAQUEdHh9LT0yVJOTk58ng88vl8\nys7OlsPhkMvl0oQJE3T8+HGlpqZe9NwAAJgmrBj4wQ9+IMuyQq8ty5LT6VRbW9tXb2y3y24/d/ud\nO3fqlVdeUVJSkp555hn5/X65XK7Qz10ul3w+37B1m80my7Lk9/uVkJAQOjYpKUk+n0+JiYlfugcx\nAADAhYUVA0eOHAn970AgII/How8++OCif9mPfvQjJSYmasqUKdqyZYs2btyom266adgxn3118Yu+\nbP1ijv2icePiZbdzGwH4KsnJzmiPAGCUhBUDn+dwODR79mxt375dv/jFLy7qvZ///MCcOXNUXl6u\ne+65Z9ijjbu6ujR16lS53W75fD5NnjxZAwMDCgaDSk5OHvbgo87OTrndbrndbn344YfnrJ9Pd3f/\nRc0OmMbnOx3tEQCMoPMFflhfLayrqxv2z8aNG9XZ2XnRgyxdulQnTpyQ9N9nF1x//fXKyMhQe3u7\nTp06pb6+Pnm9XmVmZmrWrFlqbGyUJDU3N2v69OmKjY1VSkqKDh48KEnas2ePsrOzNWPGDO3bt0+B\nQECdnZ3q6uricckAAIQprCsD77777rDXV155pdavX3/e9xw+fFjPP/+8Ojo6ZLfb1dTUpMLCQi1f\nvlyXX3654uPjVVlZqbi4OJWUlKioqEiWZam4uFhOp1Nz585Va2urFi1aJIfDobVr10qSysrKtHLl\nSg0NDSkjI0NZWVmSpPz8fBUWFsqyLJWXl8tm468zAwAQDisYzg32/6+np0eWZemqq66K5EyjIlKX\nQJetq4/IvsBo2/DkvGiPAGAEne82QVhXBrxer1asWKG+vj4Fg0ElJiZq3bp1SktLG7EhAQBAdIQV\nA9XV1frtb3+rG264QZL097//XRUVFXrttdciOhwAAIi8sG6s22y2UAhI/33uAE/3AwBgbAg7Bpqa\nmtTb26ve3l41NDQQAwAAjBFh3SZYvXq11qxZo6efflo2m02TJ0/Ws88+G+nZAADAKAjrysCf//xn\nORwO/fWvf1VbW5uGhobU0tIS6dkAAMAoCCsG6uvrtXHjxtDr7du3680334zYUAAAYPSEFQODg4PD\nPiPAA30AABg7wvrMwJw5c7Rw4ULdfPPNGhoa0oEDB3T33XdHejYAADAKwoqBxx57TLfeeqvee+89\nWZalVatWaerUqZGeDQAAjIKw/2phZmamMjMzIzkLAACIAm7+AwBgOGIAAADDEQMAABiOGAAAwHDE\nAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBwxAAAAIYjBgAAMBwxAACA\n4YgBAAAMRwwAAGA4YgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIA\nAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwXERj4OjRo7rrrru0c+dOSdJHH32khx56SAUFBVq2bJkC\ngYAkqb6+Xj/+8Y+1YMECvfHGG5KkgYEBlZSUaNGiRSosLNSJEyckSUeOHNHChQu1cOFCrVq1KvS7\ntm3bpry8PC1YsEAtLS2RPC0AAMaUiMVAf3+/1qxZo5kzZ4bWXnzxRRUUFGjXrl269tprVVdXp/7+\nfm3atEmvvvqqduzYoZqaGvX09Oitt95SQkKCXn/9dT366KOqrq6WJFVUVKisrEy7d+9Wb2+vWlpa\ndOLECTU0NGjXrl3avHmzKisrNTg4GKlTAwBgTIlYDDgcDm3dulVutzu01tbWpjvvvFOSlJOTI4/H\no0OHDiktLU1Op1NxcXGaNm2avF6vPB6PcnNzJUlZWVnyer0KBALq6OhQenr6sD3a2tqUnZ0th8Mh\nl8ulCRMm6Pjx45E6NQAAxpSIxYDdbldcXNywtTNnzsjhcEiSkpKS5PP55Pf75XK5Qse4XK5z1m02\nmyzLkt/vV0JCQujYC+0BAAAuzB6tXxwMBv/n9Yvd4/PGjYuX3R5zweMAUyUnO6M9AoBRMqoxEB8f\nr7NnzyouLk6dnZ1yu91yu93y+/2hY7q6ujR16lS53W75fD5NnjxZAwMDCgaDSk5OVk9PT+jYz+/x\n4YcfnrN+Pt3d/SN/gsAY4vOdjvYIAEbQ+QJ/VL9amJWVpaamJknSnj17lJ2drYyMDLW3t+vUqVPq\n6+uT1+tVZmamZs2apcbGRklSc3Ozpk+frtjYWKWkpOjgwYPD9pgxY4b27dunQCCgzs5OdXV1adKk\nSaN5agAAXLIidmXg8OHDev7559XR0SG73a6mpia98MILKi0tVW1trcaPH6/58+crNjZWJSUlKioq\nkmVZKi4ultPp1Ny5c9Xa2qpFixbJ4XBo7dq1kqSysjKtXLlSQ0NDysjIUFZWliQpPz9fhYWFsixL\n5eXlstl4hAIAAOGwguHcYB+DInUJdNm6+ojsC4y2DU/Oi/YIAEbQN+Y2AQAA+OYhBgAAMBwxAACA\n4YgBAAAMRwwAAGA4YgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIA\nAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBw\nxAAAAIYjBgAAMBwxAACA4YgBAAAMRwwAAGA4YgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAA\ngOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHDEAAAAhrOP5i9ra2vTsmXLdP3110uSbrjhBj38\n8MNasWKFBgcHlZycrHXr1snhcKi+vl41NTWy2WzKz8/XggULNDAwoNLSUp08eVIxMTGqrKzUxIkT\ndeTIEZWXl0uSUlNTtXr16tE8LQAALmmjfmXg1ltv1Y4dO7Rjxw4988wzevHFF1VQUKBdu3bp2muv\nVV1dnfr7+7Vp0ya9+uqr2rFjh2pqatTT06O33npLCQkJev311/Xoo4+qurpaklRRUaGysjLt3r1b\nvb29amlpGe3TAgDgkhX12wRtbW268847JUk5OTnyeDw6dOiQ0tLS5HQ6FRcXp2nTpsnr9crj8Sg3\nN1eSlJWVJa/Xq0AgoI6ODqWnpw/bAwAAhGdUbxNI0vHjx/Xoo4/q448/1pIlS3TmzBk5HA5JUlJS\nknw+n/x+v1wuV+g9LpfrnHWbzSbLsuT3+5WQkBA69rM9AABAeEY1Br73ve9pyZIluvfee3XixAn9\n5Cc/0eDgYOjnwWDwS993MetfdewXjRsXL7s9JqxjARMlJzujPQKAUTKqMXDNNddo7ty5kqTvfve7\nuvrqq9Xe3q6zZ88qLi5OnZ2dcrvdcrvd8vv9ofd1dXVp6tSpcrvd8vl8mjx5sgYGBhQMBpWcnKye\nnp7QsZ/tcSHd3f0jf4LAGOLznY72CABG0PkCf1Q/M1BfX6/f/e53kiSfz6d///vfeuCBB9TU1CRJ\n2rNnj7Kzs5WRkaH29nadOnVKfX198nq9yszM1KxZs9TY2ChJam5u1vTp0xUbG6uUlBQdPHhw2B4A\nACA8o3plYM6cOXriiSe0d+9eDQwMqLy8XFOmTNFTTz2l2tpajR8/XvPnz1dsbKxKSkpUVFQky7JU\nXFwsp9OpuXPnqrW1VYsWLZLD4dDatWslSWVlZVq5cqWGhoaUkZGhrKys0TwtAAAuaVYw3JvsY0yk\nLoEuW1cfkX2B0bbhyXnRHgHACPrG3CYAAADfPMQAAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIA\nAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhiAAAAwxEDAAAYjhgAAMBw\nxAAAAIYjBgAAMBwxAACA4YgBAAAMRwwAAGA4YgAAAMMRAwAAGI4YAADAcMQAAACGIwYAADAcMQAA\ngOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHDEAAAAhiMGAAAwHDEAAIDhiAEAAAxHDAAAYDhi\nAAAAwxEDAAAYjhgAAMBw9mgPMJKee+45HTp0SJZlqaysTOnp6dEeCQCAb7wxEwN/+ctf9M9//lO1\ntbX6xz/+obKyMtXW1kZ7LAAAvvHGzG0Cj8eju+66S5J03XXX6eOPP1Zvb2+UpwIA4JtvzFwZ8Pv9\nuvHGG0OvXS6XfD6frrzyyihOBWC0PPnW09EeARgR6+57dtR/55iJgS8KBoPn/XlysjMiv3dX1eKI\n7Avg/F79vw3RHgG4ZI2Z2wRut1t+vz/0uqurS8nJyVGcCACAS8OYiYFZs2apqalJkvT+++/L7XZz\niwAAgDCMmdsE06ZN04033qiFCxfKsiytWrUq2iMBAHBJsIIXurkOAADGtDFzmwAAAHw9xAAAAIYj\nBnBJee655/Tggw9q4cKFeu+996I9DmCUo0eP6q677tLOnTujPQpG2Jj5ACHGPh45DURPf3+/1qxZ\no5kzZ0Z7FEQAVwZwyeCR00D0OBwObd26VW63O9qjIAKIAVwy/H6/xo0bF3r92SOnAUSe3W5XXFxc\ntMdAhBADuGTxrVgAGBnEAC4ZPHIaACKDGMAlg0dOA0Bk8ARCXFJeeOEFHTx4MPTI6cmTJ0d7JMAI\nhw8f1vPPP6+Ojg7Z7XZdc801eumll5SYmBjt0TACiAEAAAzHbQIAAAxHDAAAYDhiAAAAwxEDAAAY\njhgAAMBw/KEiAP+Trq4uVVVV6ejRo7riiiskSUuXLtW//vUvtba26oUXXojyhAAuhBgA8LUFg0EV\nFxdr/vz5of/T/+CDD/Szn/1My5cvj/J0AMJFDAD42jwejyzL0uLFi0Nrqampamho0N69e0Nrf/zj\nH7Vt2zY5HA4NDg6qqqpK3/nOd1RTU6P6+npdfvnliouL07p16xQIBPTEE09Iks6ePasHH3xQeXl5\no35ugEmIAQBf27Fjx5SWlnbO+lVXXTXs9alTp/Sb3/xG48eP1+bNm/Xaa6/pqaee0osvvqimpiZd\nffXVeuedd9TV1SWPx6OUlBStXr1an3zyid54443ROh3AWMQAgK8tJiZGg4ODFzzu6quv1lNPPaVg\nMCifz6ebbrpJkpSXl6eHH35Y99xzj374wx/q+9//vux2u3bt2qXS0lLNnj1bDz74YKRPAzAe3yYA\n8LXdcMMN+tvf/nbO+gcffKAzZ85IkgYGBrR8+XKtWbNGO3fu1EMPPRQ67le/+pU2bdqkq666SsXF\nxWppadF1112nP/zhD5o3b548Hs+w4wFEBjEA4Gu79dZbdcUVV2jLli2htWPHjumXv/ylYmJiJEl9\nfX2y2WyaMGGCPvnkE+3du1eBQEAff/yxXnrpJX37299WQUGBFi9erPb2dr355ptqb29XVlaWVq1a\npY8++kiffvpptE4RMAK3CQD8T7Zs2aLKykrdd999SkxM1GWXXab169fr+PHjkqTExETdd999ysvL\n0/jx41VUVKQVK1aotbVVfX19ysvLU0JCgux2uyoqKvSf//xHq1atksPhUDAY1M9//nPZ7fynCogk\n/mohAACG4zYBAACGIwYAADAcMQAAgOGIAQAADEcMAABgOGIAAADDEQMAABiOGAAAwHD/D6Lm5SGr\nAEj0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f519906c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the target variable distribution\n",
    "sns.countplot(\"Class\",data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAF0CAYAAACAMVX9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACwCEEMAADAIv8/lH9FtelrdBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f519909c828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seperate the 2 classes into 2 Series\n",
    "Fraud_transacation = data[data[\"Class\"]==1]\n",
    "Normal_transacation= data[data[\"Class\"]==0]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(121)\n",
    "\n",
    "# amount in fraud transaction and it's frequency\n",
    "Fraud_transacation.Amount.plot.hist(title=\"Fraud Transacation\")\n",
    "plt.subplot(122)\n",
    "\n",
    "# amount in normal transaction and it's frequency\n",
    "Normal_transacation.Amount.plot.hist(title=\"Normal Transaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.iloc[:,data.columns != \"Class\"]\n",
    "target = data.iloc[:,data.columns==\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787   ...    0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654   ...    0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see also: https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99955525906159659"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85293,     3,    35,   112])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.97      0.76      0.85       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('autoencoder_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).to_csv('autoencoder_target_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading CSV input...\n",
      "finished reading data...\n",
      "performing train/validation split...\n",
      "finished performing train/validation split...\n",
      "creating placeholders for input and output...\n",
      "creating W,b,h variables for various layers...\n",
      "\tnow creating input layer...\n",
      "\t\t 30 , 12\n",
      "\tnow creating layer: 1\n",
      "\t\t 12 , 7\n",
      "\tnow creating layer: 2\n",
      "\t\t 7 , 3\n",
      "\tnow creating layer: 3\n",
      "\t\t 3 , 7\n",
      "\tnow creating layer: 4\n",
      "\t\t 7 , 12\n",
      "\tnow creating output layer...\n",
      "\t\t 12 , 30\n",
      "\tsetting up output vector...\n",
      "defining objective function...\n",
      "defining optimization step...\n",
      "defining optimization step...\n",
      "training_accuracy => 25873.1543 for step 0\n",
      "training_accuracy => 24980.7168 for step 1\n",
      "training_accuracy => 23682.4277 for step 2\n",
      "training_accuracy => 23076.8281 for step 3\n",
      "training_accuracy => 23377.1074 for step 4\n",
      "training_accuracy => 12029.4502 for step 5\n",
      "training_accuracy => 14735.6631 for step 6\n",
      "training_accuracy => 6981.8140 for step 7\n",
      "training_accuracy => 8184.2695 for step 8\n",
      "training_accuracy => 9603.7637 for step 9\n",
      "training_accuracy => 9333.0068 for step 10\n",
      "training_accuracy => 14731.3633 for step 20\n",
      "training_accuracy => 13745.6465 for step 30\n",
      "training_accuracy => 9386.1357 for step 40\n",
      "training_accuracy => 7636.3477 for step 50\n",
      "training_accuracy => 13486.3896 for step 60\n",
      "training_accuracy => 28523.3828 for step 70\n",
      "training_accuracy => 15509.6768 for step 80\n",
      "training_accuracy => 7119.1924 for step 90\n",
      "training_accuracy => 25568.0547 for step 100\n",
      "training_accuracy => 11682.5547 for step 200\n",
      "training_accuracy => 26918.3770 for step 300\n",
      "training_accuracy => 20349.9004 for step 400\n",
      "training_accuracy => 9910.1201 for step 500\n",
      "training_accuracy => 23298.4785 for step 600\n",
      "training_accuracy => 9380.0986 for step 700\n",
      "training_accuracy => 10739.0352 for step 800\n",
      "training_accuracy => 7819.8271 for step 900\n",
      "training_accuracy => 5737.3789 for step 1000\n",
      "training_accuracy => 14333.1455 for step 2000\n",
      "training_accuracy => 2011.6992 for step 3000\n",
      "training_accuracy => 13865.0205 for step 4000\n",
      "training_accuracy => 12833.6982 for step 5000\n",
      "training_accuracy => 23988.5703 for step 6000\n",
      "training_accuracy => 6968.3340 for step 7000\n",
      "training_accuracy => 20873.6875 for step 8000\n",
      "training_accuracy => 13280.2207 for step 9000\n",
      "training_accuracy => 12317.3203 for step 10000\n",
      "training_accuracy => 29936.6094 for step 19999\n",
      "W:\n",
      "[[  7.37022981e-02   1.50217757e-01   7.63339177e-02  -5.06384373e-02\n",
      "   -4.37331619e-04   2.43531894e-02  -5.76099344e-02   1.91270202e-01\n",
      "   -1.42900392e-01  -7.89549872e-02  -9.80363041e-02  -1.41835976e-02]\n",
      " [  3.57251167e-02  -7.27149621e-02   6.21627904e-02  -6.62549436e-02\n",
      "   -4.77286167e-02   1.67187843e-02   4.97326963e-02  -1.03455722e-01\n",
      "   -9.53381434e-02   1.24390006e-01  -1.77381337e-01   6.84993565e-02]\n",
      " [ -8.07237029e-02   3.98092866e-02  -4.63516191e-02   5.92426062e-02\n",
      "    1.48697853e-01   2.18437109e-02   1.44340908e-02  -2.62724776e-02\n",
      "    1.86480716e-01   8.64242315e-02   5.59989316e-03   1.35527730e-01]\n",
      " [ -1.36082560e-01   7.30122477e-02   6.95673376e-02  -8.07145163e-02\n",
      "   -8.76238942e-03   1.08481711e-02  -7.64971692e-03  -6.03886554e-03\n",
      "    1.47628747e-02  -7.13763759e-02  -1.75786018e-02  -1.45310555e-02]\n",
      " [  6.08685426e-02  -6.74204305e-02   4.70520221e-02   2.38617114e-03\n",
      "   -6.58726841e-02   8.88623223e-02   1.75210848e-01   5.86261190e-02\n",
      "    1.60642996e-01   1.37318060e-01   1.60633866e-02  -1.08066738e-01]\n",
      " [ -5.89074790e-02  -2.79583838e-02  -9.48150009e-02  -1.43434172e-02\n",
      "   -6.76935166e-02  -1.07883886e-01   8.08040500e-02   1.50019690e-01\n",
      "   -1.14514023e-01   2.39781642e-04   1.12001359e-01   2.43481919e-02]\n",
      " [ -1.03419542e-01  -7.48004317e-02  -1.27500951e-01   9.70253274e-02\n",
      "    3.12113017e-02   1.12879030e-01  -8.07010382e-02  -1.44703805e-01\n",
      "   -6.24877661e-02   9.98903811e-03  -9.29235816e-02  -4.38099243e-02]\n",
      " [ -2.79692840e-02  -1.25205770e-01  -8.88326168e-02   3.59278396e-02\n",
      "   -3.86651829e-02   1.00451298e-01   3.01132677e-03   8.63527954e-02\n",
      "   -4.47504073e-02  -7.63063058e-02   4.39195707e-02   3.06799188e-02]\n",
      " [ -8.98367837e-02  -1.41689673e-01   3.14432196e-02   2.25951131e-02\n",
      "   -4.17084023e-02   1.60029918e-01   9.71231461e-02   1.77408129e-01\n",
      "    8.72266069e-02  -6.73727542e-02   9.23204273e-02   2.08353512e-02]\n",
      " [  1.30284876e-01  -2.07751766e-02   9.29077342e-02   1.21167891e-01\n",
      "    1.03222497e-01   5.17679378e-02  -1.14076853e-01   1.55234650e-01\n",
      "   -1.07671291e-01   3.48155014e-02  -7.41212294e-02  -1.39771309e-02]\n",
      " [  1.06420510e-01   1.02191970e-01  -8.66345614e-02   9.41636041e-02\n",
      "   -9.03854072e-02   6.49260432e-02  -1.32828161e-01  -6.41789660e-02\n",
      "   -1.01988740e-01   1.06788628e-01  -8.18860307e-02   7.45685399e-02]\n",
      " [ -1.38348296e-01   4.64496911e-02  -1.01885445e-01   9.60119441e-02\n",
      "   -7.35714473e-03   4.14062925e-02   8.44058543e-02   5.88856339e-02\n",
      "   -1.28334478e-01   1.24931261e-02  -3.36710783e-03  -7.30168670e-02]\n",
      " [ -1.67213455e-01   6.41333982e-02   1.32427454e-01   1.31459728e-01\n",
      "    4.42128181e-02  -5.21342605e-02  -8.53074268e-02  -9.67977345e-02\n",
      "   -6.03621900e-02   1.75761029e-01  -7.33196456e-03  -1.22723989e-01]\n",
      " [  3.65896225e-02  -4.63159718e-02  -4.09875289e-02   7.03476369e-02\n",
      "    5.38901463e-02  -9.12183374e-02  -5.53919971e-02   1.12116836e-01\n",
      "   -1.55933276e-01  -1.82080731e-01  -6.30554259e-02  -8.93418193e-02]\n",
      " [  8.90171751e-02   6.67827353e-02   6.24694638e-02  -9.36335996e-02\n",
      "   -1.45385163e-02   1.39409713e-02   1.97566510e-03   1.45824924e-01\n",
      "    2.38221418e-02  -8.43756050e-02   7.80523047e-02  -4.20586988e-02]\n",
      " [ -1.10124685e-01  -5.61109781e-02  -1.54636130e-01   3.70116271e-02\n",
      "    1.89387560e-01  -1.13852553e-01   9.01694875e-03   6.60756975e-02\n",
      "    1.73775423e-02   8.15159678e-02  -9.92623251e-03   9.27395523e-02]\n",
      " [ -6.23369105e-02   9.25521553e-02  -5.59585914e-02  -1.13325708e-01\n",
      "   -4.08071093e-02  -8.76666754e-02   8.38045031e-02   5.41748591e-02\n",
      "   -4.04432509e-03  -5.70534285e-05   5.90987504e-02  -1.16449937e-01]\n",
      " [  8.71749371e-02   1.60874665e-01   5.07425256e-02   9.19835363e-03\n",
      "    1.93849668e-01  -4.30497155e-02   2.62139048e-02   8.96508098e-02\n",
      "    9.72353742e-02   6.67923922e-03   2.35917848e-02  -2.00738590e-02]\n",
      " [  1.93850651e-01   6.88378140e-02  -7.09709227e-02   7.37380311e-02\n",
      "   -8.51957574e-02   6.64411411e-02   1.54205039e-02  -1.41956106e-01\n",
      "    2.16881074e-02   5.92772923e-02   7.34745711e-03  -3.69478725e-02]\n",
      " [  1.74481422e-01  -2.26183608e-02   1.26140565e-01  -3.89276855e-02\n",
      "   -1.05603874e-01  -1.19257890e-01  -9.39816609e-02   3.71943414e-02\n",
      "   -1.95529722e-02  -8.89167488e-02  -5.00560412e-03   7.28952140e-02]\n",
      " [ -1.13142962e-02   4.10673916e-02  -1.19090669e-01  -7.60027627e-03\n",
      "    9.78574231e-02   1.14147194e-01   1.29598200e-01  -1.41269416e-01\n",
      "    1.49494663e-01  -1.62873808e-02  -7.32468367e-02   7.38762617e-02]\n",
      " [ -4.85586338e-02  -5.59725314e-02  -1.24636786e-02  -6.15902012e-03\n",
      "    5.76023795e-02  -1.97838038e-01  -4.57283705e-02   2.82069817e-02\n",
      "   -3.60554010e-02  -5.35469837e-02  -8.07922781e-02  -5.92950322e-02]\n",
      " [  5.86406095e-03   1.09409660e-01  -9.81380567e-02  -1.30613357e-01\n",
      "    7.81982690e-02   7.55520910e-02  -4.41977791e-02   8.43223929e-02\n",
      "   -2.59576421e-02   9.28450897e-02  -1.37555292e-02   1.16029084e-01]\n",
      " [ -3.80472653e-03  -1.31262973e-01   1.61299661e-01  -1.05554238e-02\n",
      "    1.09321758e-01  -2.75091995e-02  -6.74991608e-02  -1.07529357e-01\n",
      "   -4.58078869e-02  -5.51448129e-02  -7.49919191e-02  -4.72486317e-02]\n",
      " [ -7.57955685e-02  -5.18332757e-02   1.17681280e-01   5.51300235e-02\n",
      "    2.26463135e-02  -6.11681230e-02  -3.63406315e-02   8.08818173e-03\n",
      "    6.23712838e-02  -1.06541909e-01   1.55231571e-02  -1.01416387e-01]\n",
      " [ -9.93382484e-02   9.17261839e-03   3.98553573e-02  -1.42907381e-01\n",
      "    1.36988357e-01  -5.20208292e-02  -4.57079802e-03  -9.41145644e-02\n",
      "    4.59175371e-02   4.72704209e-02   1.00286817e-02  -1.45450115e-01]\n",
      " [ -1.05795048e-01   1.40410513e-01  -3.99902910e-02   5.57454489e-02\n",
      "   -5.86798303e-02  -4.70024645e-02   3.54559720e-02  -5.49407825e-02\n",
      "    5.50321750e-02  -1.69515759e-01   1.41525269e-02  -7.74271488e-02]\n",
      " [  8.48246738e-03  -6.91509098e-02  -1.90959185e-01   9.13471207e-02\n",
      "    5.33049228e-03   1.08277500e-01   6.49448037e-02  -4.75900620e-02\n",
      "    1.20430836e-03   1.53727306e-03   9.47103426e-02   3.15460749e-02]\n",
      " [  1.26239643e-01  -8.25663880e-02   1.02299094e-01   9.86112505e-02\n",
      "   -1.64492235e-01  -3.91562507e-02  -1.03800237e-01  -9.20306891e-02\n",
      "    6.18290156e-02   1.04269482e-01   1.28533602e-01  -1.67518551e-03]\n",
      " [  9.47414413e-02   5.64350039e-02   4.35511917e-02  -1.45352995e-02\n",
      "   -7.21455691e-03  -1.47541738e-04  -7.82362297e-02  -1.25074178e-01\n",
      "    1.98080659e-01  -3.96148674e-02  -7.72861913e-02   1.20628446e-01]]\n",
      "b:\n",
      "[ 0.09754101  0.09808207  0.10241282  0.1         0.09889809  0.09874838\n",
      "  0.1         0.10215439  0.09986649  0.1         0.1         0.09862148]\n",
      "W:\n",
      "[[-0.04304013 -0.09454754 -0.01222148 -0.02246846 -0.00909736  0.18592681\n",
      "  -0.09023251]\n",
      " [ 0.00759309  0.08483417 -0.12585641  0.11500358 -0.0921316   0.10784691\n",
      "  -0.13344786]\n",
      " [ 0.01526968  0.05526859 -0.00118714  0.0173686  -0.06330658  0.04981513\n",
      "   0.12066355]\n",
      " [-0.0349175  -0.01100388  0.12654029 -0.08698344 -0.15161091  0.03205892\n",
      "  -0.02712929]\n",
      " [-0.03391041  0.10283361  0.0201021   0.10550807 -0.04972009  0.03835762\n",
      "  -0.08668203]\n",
      " [-0.0055866  -0.0808834  -0.03235301 -0.05300669  0.15766163 -0.00774352\n",
      "  -0.02899132]\n",
      " [ 0.01424132 -0.13478908 -0.00895137 -0.14669226  0.00365141 -0.09464721\n",
      "   0.1008847 ]\n",
      " [-0.021523   -0.06337496  0.0752626  -0.14801796  0.0439905   0.11899077\n",
      "   0.12699722]\n",
      " [-0.08097494  0.10008966  0.091801   -0.11515246 -0.18308918  0.10283496\n",
      "   0.03858745]\n",
      " [-0.02375528  0.07997615 -0.1041128   0.06891433 -0.04534357 -0.05731049\n",
      "   0.05744311]\n",
      " [ 0.01950968 -0.10656809  0.10679749  0.06379946  0.10189395  0.08077794\n",
      "   0.12323536]\n",
      " [ 0.18518573  0.03409575 -0.14460978  0.04280988 -0.02951609 -0.08769844\n",
      "   0.05727845]]\n",
      "b:\n",
      "[ 0.09941436  0.10070754  0.1         0.10021104  0.09984159  0.10001657\n",
      "  0.10531006]\n",
      "W:\n",
      "[[-0.05315579  0.08788863 -0.05009935]\n",
      " [-0.07106036  0.07324344  0.15196674]\n",
      " [-0.08815572  0.05861316  0.05321346]\n",
      " [ 0.10499429 -0.0148894   0.06035987]\n",
      " [-0.02391382 -0.00465972 -0.12030801]\n",
      " [-0.06870133 -0.13826603  0.06912283]\n",
      " [ 0.10162805  0.06519882  0.09213041]]\n",
      "b:\n",
      "[ 0.10011082  0.09984148  0.10037458]\n",
      "W:\n",
      "[[-0.09770685 -0.08120664  0.09526041  0.06509373 -0.09953401  0.14266714\n",
      "  -0.01228932]\n",
      " [ 0.03953093 -0.0074552   0.06931821 -0.08869074 -0.0420674   0.07945031\n",
      "  -0.00499058]\n",
      " [-0.1318844  -0.10803577 -0.01752435 -0.00613085 -0.04643418 -0.0457408\n",
      "  -0.07622062]]\n",
      "b:\n",
      "[ 0.09823617  0.09757981  0.09171632  0.07738163  0.09925854  0.10415026\n",
      "  0.0987536 ]\n",
      "W:\n",
      "[[ 0.08436597 -0.1156453  -0.07732765 -0.02746268 -0.12324358  0.00167041\n",
      "   0.02059247  0.15386964  0.04936419 -0.02485744 -0.00881442 -0.04190731]\n",
      " [-0.01124103 -0.06108017  0.08455036 -0.03754177  0.00502112 -0.0160334\n",
      "  -0.08857282 -0.05716849  0.06783921 -0.0474484   0.13428834 -0.14949152]\n",
      " [-0.05308769  0.03247267 -0.00818013 -0.00439667  0.05275833 -0.09623269\n",
      "   0.11068939 -0.03107858  0.00934159 -0.16171661  0.02748776 -0.01842819]\n",
      " [-0.02201811 -0.04331643 -0.06457731  0.17354511  0.07760648 -0.024915\n",
      "  -0.01541352 -0.01997334  0.01318143 -0.05881658 -0.05151182  0.08841202]\n",
      " [-0.03999376  0.06533337 -0.14078493  0.1080634   0.06961516  0.03862622\n",
      "   0.09750728  0.03777677  0.013849    0.07340101  0.0367236   0.08807515]\n",
      " [-0.09015127  0.12388577 -0.04562904 -0.06196914  0.01107911 -0.09206472\n",
      "  -0.04674464 -0.08578928  0.06702596  0.07547199 -0.08008199 -0.04609247]\n",
      " [ 0.13667946  0.04623602  0.04433347  0.02961541  0.06903072 -0.16734323\n",
      "  -0.01241923 -0.13572589  0.15830892 -0.14909792  0.0452865   0.02702012]]\n",
      "b:\n",
      "[ 0.20185287  0.2015762   0.2047969   0.02335568  0.01107247  0.204153\n",
      "  0.00434152  0.20498538  0.20559686  0.20855239  0.05015639  0.15270887]\n",
      "W:\n",
      "[[  9.81470719e-02  -1.29029965e-02  -3.30523923e-02   9.88072753e-02\n",
      "   -7.29134977e-02   2.76083928e-02   7.47210234e-02   7.98308030e-02\n",
      "   -2.64224857e-02   9.95459855e-02   1.00622043e-01   5.60118631e-03\n",
      "    1.01131760e-01  -4.72186208e-02  -6.70092031e-02   1.23597041e-03\n",
      "   -1.11060604e-01   1.68249235e-01   5.44430949e-02   1.79005563e-01\n",
      "   -1.41963109e-01  -2.08172482e-02   1.32554218e-01  -2.92213876e-02\n",
      "   -1.10190228e-01   9.38190147e-02  -1.37695119e-01   1.44908592e-01\n",
      "   -5.35247810e-02   3.82775515e-02]\n",
      " [  2.98685193e-01   1.50138717e-02  -1.90619320e-01   5.07121533e-02\n",
      "   -5.48172668e-02  -7.32890815e-02  -5.12583964e-02  -5.21269180e-02\n",
      "    1.34550631e-01  -1.01404652e-01   8.62149615e-03   1.01832017e-01\n",
      "    3.61691117e-02  -6.14184923e-02  -4.96411249e-02  -2.02881098e-02\n",
      "   -4.94144903e-03   4.43923809e-02   1.29309921e-02   8.05916414e-02\n",
      "    7.48354793e-02   1.55099884e-01  -3.63253094e-02  -3.70687321e-02\n",
      "    2.54686251e-02  -1.39545858e-01   2.01072041e-02   3.86719145e-02\n",
      "   -1.24991566e-01   9.68393236e-02]\n",
      " [  1.42443791e-01   8.21423233e-02   1.00491941e-02  -4.51103076e-02\n",
      "    8.61987323e-02  -1.56205907e-01   5.28784549e-05   6.76179957e-03\n",
      "    3.60457157e-03   1.76377580e-01  -5.28484099e-02   5.77719994e-02\n",
      "    1.49706844e-02   1.99226901e-01  -4.46548546e-03  -6.16188198e-02\n",
      "   -1.75459445e-01  -1.54534623e-01  -1.17760837e-01  -1.01698659e-01\n",
      "   -2.60697491e-02   1.67012036e-01   1.76820964e-01  -2.80287173e-02\n",
      "    9.64713916e-02   9.22021735e-03   9.72607955e-02   9.70953479e-02\n",
      "   -1.68535206e-02   6.06813915e-02]\n",
      " [  5.33646578e-03   1.54392377e-01   1.87332585e-01   1.91142797e-01\n",
      "   -1.45899435e-03  -1.20748868e-02   1.00674875e-01  -9.44532976e-02\n",
      "   -2.77724545e-02  -2.36548316e-02  -7.75085017e-02   6.73888549e-02\n",
      "   -5.96627705e-02  -8.92141610e-02   3.40980478e-02  -1.56229809e-01\n",
      "    8.93922448e-02  -2.79682204e-02  -5.49496971e-02   7.19119087e-02\n",
      "   -3.88988480e-02   3.21579841e-03  -6.48233593e-02   1.72084395e-03\n",
      "    7.26539120e-02   1.21444575e-01  -1.29485592e-01   1.23441021e-03\n",
      "    5.85952066e-02  -1.43307112e-02]\n",
      " [ -7.68503966e-03   5.81621341e-02  -7.43264705e-02  -1.14808965e-03\n",
      "   -1.06328232e-02  -7.22102523e-02   1.17346216e-02   1.05328165e-01\n",
      "    1.18033970e-02   6.08886592e-02  -3.79465222e-02  -9.60474759e-02\n",
      "   -1.02220826e-01  -3.81403714e-02   9.37466696e-03   8.97391234e-03\n",
      "   -1.57919731e-02   3.25214826e-02   8.94508958e-02   1.14659503e-01\n",
      "   -7.51832947e-02  -1.56782225e-01   1.07077807e-02  -1.58292726e-01\n",
      "   -9.91567001e-02   6.06553890e-02   1.85563639e-02  -8.01507011e-02\n",
      "    9.91643174e-04   9.45050120e-02]\n",
      " [  1.68882579e-01  -1.76717818e-01   5.44802360e-02   6.21536933e-02\n",
      "    6.10763580e-02  -1.71628952e-01  -1.78130329e-01   1.44075947e-02\n",
      "    8.46785586e-03   2.27510296e-02  -5.83384708e-02  -4.96355928e-02\n",
      "    6.39190078e-02   6.85165972e-02   1.44104183e-01  -3.15512195e-02\n",
      "    9.46060866e-02  -4.61127870e-02  -3.18034478e-02   7.14830160e-02\n",
      "    2.28428617e-02  -3.59831378e-04  -9.23066437e-02  -9.55966040e-02\n",
      "   -1.39468849e-01  -1.30140066e-01   3.09525011e-03   3.46931443e-02\n",
      "    4.77453731e-02  -2.24802606e-02]\n",
      " [ -5.84422499e-02  -1.25924170e-01   9.04164612e-02  -1.15239635e-01\n",
      "    9.04197022e-02  -1.84849696e-03   1.77377209e-01  -1.84378967e-01\n",
      "    9.33484640e-03   7.31329098e-02  -7.83991665e-02  -2.42852662e-02\n",
      "   -1.41446320e-02  -8.23276490e-03  -5.89007698e-02   6.00741841e-02\n",
      "    8.75490308e-02  -1.00967772e-01  -3.52979638e-02   9.09685269e-02\n",
      "    9.80754644e-02  -2.38664169e-02  -2.73701455e-02   7.11241215e-02\n",
      "   -1.05332606e-01   1.73979178e-01  -4.67091016e-02   1.24861464e-01\n",
      "   -9.53323916e-02   1.00835711e-01]\n",
      " [  1.53375894e-01   5.32735512e-02   1.00649633e-01  -1.15474038e-01\n",
      "   -3.37299742e-02  -5.00678504e-03  -9.75551903e-02   3.27393115e-02\n",
      "    1.22146405e-01  -2.51393765e-02   9.38655511e-02  -6.21454529e-02\n",
      "   -1.91016421e-02  -1.06384836e-01   4.07782104e-03  -8.33377317e-02\n",
      "   -7.43633881e-02   1.37916282e-01   1.31251633e-01   1.32873595e-01\n",
      "    3.30222771e-02  -1.72929496e-01  -1.34668490e-02  -1.00328892e-01\n",
      "    1.12689450e-01   1.30313113e-01   3.41401808e-02  -9.74043459e-02\n",
      "   -2.96568014e-02  -5.25778830e-02]\n",
      " [  1.41623542e-01  -8.27530175e-02  -9.08239409e-02   3.13031040e-02\n",
      "    1.73364744e-01  -1.89798709e-03   9.83108506e-02  -1.16704859e-01\n",
      "   -4.86174412e-02   1.00842327e-01  -1.36271253e-01  -4.29408774e-02\n",
      "   -1.10676922e-02  -7.22002760e-02   7.82016218e-02   2.23523583e-02\n",
      "   -1.16888754e-01   3.22227962e-02   3.00955120e-02   8.07714611e-02\n",
      "   -3.29290740e-02   2.74306610e-02  -6.37724297e-03   1.28120948e-02\n",
      "    3.99672538e-02   1.95524588e-01   1.73067242e-01   3.91027853e-02\n",
      "    4.80202921e-02   2.40215892e-03]\n",
      " [  1.12601846e-01  -1.46897621e-02  -2.08486617e-02   4.14594300e-02\n",
      "   -2.54745726e-02   1.73138827e-01   7.01092109e-02  -1.07767850e-01\n",
      "   -2.91730352e-02  -1.12947062e-01  -1.08727410e-01  -3.77371870e-02\n",
      "    1.55796424e-01   4.25303020e-02  -1.24551386e-01  -8.86479579e-03\n",
      "    1.10605247e-01   5.95032983e-02  -9.69812647e-02   9.10146385e-02\n",
      "   -4.09392342e-02  -1.53540254e-01   5.19834906e-02   1.21581219e-01\n",
      "   -1.25053888e-02   9.13196728e-02  -1.02610402e-01  -2.98031718e-02\n",
      "    7.77132362e-02  -6.08439445e-02]\n",
      " [  1.80381481e-02   5.03712408e-02   2.75363438e-02   1.64890606e-02\n",
      "   -2.50498979e-05   7.66825071e-03   1.29911870e-01  -4.37256955e-02\n",
      "   -5.72207347e-02  -1.11660860e-01   3.82123701e-02  -7.60115534e-02\n",
      "   -9.24202725e-02   3.77640054e-02  -5.53590283e-02   1.08323477e-01\n",
      "   -5.81454821e-02   3.34516540e-02   3.27548273e-02   5.11869614e-04\n",
      "    1.07772490e-02   1.07305959e-01   1.71383962e-01  -1.65463954e-01\n",
      "    1.22674599e-01  -2.57146880e-02   1.14516236e-01  -5.49646579e-02\n",
      "   -4.02609669e-02  -3.43314633e-02]\n",
      " [  7.06477165e-02   2.88597103e-02  -1.29416689e-01  -3.03058419e-02\n",
      "   -2.14556716e-02  -2.91114710e-02  -4.21943553e-02  -6.12684786e-02\n",
      "   -2.00846314e-01  -1.04092374e-01  -4.35444005e-02  -8.79386291e-02\n",
      "    8.15427676e-02   1.57703057e-01  -1.09134562e-01   5.26951216e-02\n",
      "   -8.33624601e-02  -1.61655657e-02   2.94625238e-02  -7.33982101e-02\n",
      "    3.43404226e-02   2.85095070e-02   3.11613809e-02  -9.88526344e-02\n",
      "   -2.18008086e-02   7.24019706e-02  -4.29785624e-02   9.59373917e-03\n",
      "    4.70346101e-02  -4.97067980e-02]]\n",
      "b:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2000715   0.09827068  0.10142931  0.10391224  0.10028894  0.09910514\n",
      "  0.09926347  0.10024861  0.09849606  0.10049939  0.0990357   0.1009963\n",
      "  0.09642813  0.09926523  0.09929165  0.09972423  0.09837814  0.09813463\n",
      "  0.09699301  0.097462    0.09819465  0.09670656  0.09618504  0.09701987\n",
      "  0.09931745  0.09766701  0.09809095  0.09588374  0.09649981  0.11337442]\n",
      "Train_x Shape ::  (133573, 30)\n",
      "Train_y Shape ::  (133573,)\n",
      "Test_x Shape ::  (65791, 30)\n",
      "Test_y Shape ::  (65791,)\n",
      "Trained model ::  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Train Accuracy ::  0.998367933639\n",
      "Test Accuracy  ::  0.998054445137\n",
      " Confusion matrix  [[65663     2]\n",
      " [  126     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     65665\n",
      "          1       0.00      0.00      0.00       126\n",
      "\n",
      "avg / total       1.00      1.00      1.00     65791\n",
      "\n",
      "Trained model ::  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train Accuracy ::  0.998360447096\n",
      "Test Accuracy  ::  0.998084844432\n",
      " Confusion matrix  [[65665     0]\n",
      " [  126     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     65665\n",
      "          1       0.00      0.00      0.00       126\n",
      "\n",
      "avg / total       1.00      1.00      1.00     65791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dipamvasani7/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import \n",
    "\n",
    "import pdb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import csv\n",
    "\n",
    "model_output = []\n",
    "\n",
    "def split_dataset(dataset, train_percentage, feature_headers, target_header):\n",
    "    \"\"\"\n",
    "    Split the dataset with train_percentage\n",
    "    :param dataset:\n",
    "    :param train_percentage:\n",
    "    :param feature_headers:\n",
    "    :param target_header:\n",
    "    :return: train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and test dataset\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[feature_headers], dataset[target_header],\n",
    "                                                        train_size=train_percentage)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def random_forest_classifier(features, target):\n",
    "    \"\"\"\n",
    "    To train the random forest classifier with features and target data\n",
    "    :param features:\n",
    "    :param target:\n",
    "    :return: trained random forest classifier\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(features, target)\n",
    "    return clf\n",
    "\n",
    "def logistic_regression_classifier(features, target):\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(features, target)\n",
    "    return log_reg\n",
    "    \n",
    "\n",
    "\n",
    "now = time.time()\n",
    "tag = str(now)\n",
    "seed = 9\n",
    "\n",
    "DEBUG = True\n",
    "# settings\n",
    "LEARNING_RATE = 5e-6\n",
    "TRAINING_ITERATIONS = 20000\n",
    "\n",
    "# sizes of the hidden layers\n",
    "NN_HL_ARCH = [12, 7, 3, 7, 12]  \n",
    "    \n",
    "# probability of keeping a neuron = 1-prob(dropout)\n",
    "DROPOUT = 1.0\n",
    "\n",
    "#\n",
    "BATCH_SIZE = 1 # there may be some errors if set to > 1. \n",
    "\n",
    "# set to 0 to train on all available data\n",
    "# (currently validation is not used)\n",
    "VALIDATION_SIZE = 0 #2000\n",
    "\n",
    "# read training data from CSV file\n",
    "if DEBUG:\n",
    "    print('reading CSV input...')\n",
    "data = pd.read_csv('autoencoder_input.csv')\n",
    "output_y = pd.read_csv('autoencoder_target_input.csv')\n",
    "y_op = output_y.Class\n",
    "\n",
    "headers = list(data)\n",
    "headertext = ','.join(headers)\n",
    "\n",
    "#def normalize(X,has_outliers=True,has_missing=True,is_sparse=True):\n",
    " #   robust_scaler = RobustScaler(with_centering=True)\n",
    "  #  data = robust_scaler.fit_transform(X)\n",
    "   # if has_outliers:\n",
    "    #    pass\n",
    "        #data = preprocessing.scale(X,with_centering=False)\n",
    "    #return data\n",
    "\n",
    "inputs = data.values\n",
    "inputs = inputs.astype(np.float)\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(inputs, y_op, test_size=0.33, random_state=seed)\n",
    "\n",
    "if DEBUG:\n",
    "    print('finished reading data...')\n",
    "\n",
    "#if DEBUG:\n",
    " #   print('normalizing data...')\n",
    "#inputs = normalize(inputs)\n",
    "\n",
    "# split data into training & validation\n",
    "# for an autoencoder labels are the same as inputs\n",
    "if DEBUG:\n",
    "    print('performing train/validation split...')\n",
    "validation_inputs = X_train[:VALIDATION_SIZE]\n",
    "validation_labels = X_train[:VALIDATION_SIZE]\n",
    "\n",
    "train_inputs = X_train[VALIDATION_SIZE:]\n",
    "train_labels = X_train[VALIDATION_SIZE:]\n",
    "\n",
    "input_size = len(X_train[0])\n",
    "\n",
    "if DEBUG:\n",
    "    print('finished performing train/validation split...')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "if DEBUG:\n",
    "    print('creating placeholders for input and output...')\n",
    "\n",
    "\n",
    "# inputs\n",
    "x = tf.placeholder('float', shape=[None,input_size])\n",
    "# outputs = labels\n",
    "y_ = tf.placeholder('float', shape=[None,input_size])\n",
    "\n",
    "# To prevent overfitting, we  apply [dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) \n",
    "# before the readout layer.\n",
    "# \n",
    "# Dropout removes some nodes from the network at each training stage. Each of the nodes is either kept in the \n",
    "# network with probability *keep_prob* or dropped with probability *1 - keep_prob*. After the training stage \n",
    "# is over the nodes are returned to the NN with their original weights.\n",
    "keep_prob = tf.placeholder('float')\n",
    "if DEBUG:\n",
    "    print('creating W,b,h variables for various layers...')\n",
    "\n",
    "\n",
    "W = []\n",
    "b = []\n",
    "h = []\n",
    "h_tmp = []\n",
    "\n",
    "if DEBUG:\n",
    "    print('\\tnow creating input layer...')\n",
    "    print('\\t\\t',input_size,',',NN_HL_ARCH[0])\n",
    "\n",
    "W.append(weight_variable([input_size,NN_HL_ARCH[0]]))\n",
    "b.append(bias_variable([NN_HL_ARCH[0]]))\n",
    "h.append(tf.nn.relu(tf.matmul(x,W[0]) + b[0]))\n",
    "\n",
    "for i in range(1,len(NN_HL_ARCH)):\n",
    "    u = NN_HL_ARCH[i-1]\n",
    "    v = NN_HL_ARCH[i]\n",
    "    if DEBUG:\n",
    "        print('\\tnow creating layer:', i)\n",
    "        print('\\t\\t',u,',',v)\n",
    "\n",
    "    W.append(weight_variable([u,v]))\n",
    "    b.append(bias_variable([v]))\n",
    "    if DEBUG:\n",
    "        pass\n",
    "        #print('h', tf.shape(h[i-1]), ', W:', tf.shape(W[i]), ', b:', tf.shape(b[i]))\n",
    "\n",
    "    h_tmp.append(tf.nn.relu(tf.matmul(h[i-1],W[i]) + b[i]))\n",
    "    # dropout\n",
    "    h.append(tf.nn.dropout(h_tmp[-1], keep_prob))\n",
    "\n",
    "if DEBUG:\n",
    "    print('\\tnow creating output layer...')\n",
    "    print('\\t\\t',NN_HL_ARCH[-1],',',input_size)\n",
    "\n",
    "W.append(weight_variable([NN_HL_ARCH[-1],input_size]))\n",
    "b.append(bias_variable([input_size]))\n",
    "if DEBUG:\n",
    "    print('\\tsetting up output vector...')\n",
    "y = tf.nn.relu(tf.matmul(h[-1],W[-1]) + b[-1])\n",
    "\n",
    "# \n",
    "# ADAM optimiser is a gradient based optimization algorithm, based on adaptive estimates, it's more \n",
    "# sophisticated than steepest gradient descent and is well suited for problems with large data or many parameters.\n",
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "if DEBUG:\n",
    "    print('defining objective function...')\n",
    "# but we shall use rmse\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.pow(y-y_, 2)))\n",
    "\n",
    "if DEBUG:\n",
    "    print('defining optimization step...')\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(rmse)\n",
    "\n",
    "# evaluation\n",
    "if DEBUG:\n",
    "    print('defining optimization step...')\n",
    "\n",
    "# CHECK\n",
    "error_sq_vector = tf.pow(y - y_,2)\n",
    "\n",
    "# CHECK\n",
    "accuracy = tf.sqrt(tf.reduce_mean(error_sq_vector))\n",
    "predict = tf.identity(y)\n",
    "\n",
    "# *Finally neural network structure is defined and TensorFlow graph is ready for training.*\n",
    "# ## Train, validate and predict\n",
    "# #### Helper functions\n",
    "# \n",
    "# Ideally, we should use all data for every step of the training, but that's expensive. So, instead, \n",
    "# we use small \"batches\" of random data. \n",
    "# \n",
    "# This method is called [stochastic training](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). \n",
    "# It is cheaper, faster and gives much of the same result.\n",
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_inputs.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_inputs\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_inputs = train_inputs[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_inputs[start:end], train_labels[start:end]\n",
    "# Now when all operations for every variable are defined in TensorFlow graph all computations \n",
    "# will be performed outside Python environment.\n",
    "# start TensorFlow session\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)\n",
    "# Each step of the loop, we get a \"batch\" of data points from the training set and feed it to \n",
    "# the graph to replace the placeholders.  In this case, it's:  *x, y* and *dropout.*\n",
    "# \n",
    "# Also, once in a while, we check training accuracy on an upcoming \"batch\".\n",
    "# \n",
    "# On the local environment, we recommend [saving training progress]\n",
    "# (https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#Saver), \n",
    "# so it can be recovered for further training, debugging or evaluation.\n",
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_inputs[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})\n",
    "# After training is done, it's good to check accuracy on data that wasn't used in training.\n",
    "# check final accuracy on validation set  \n",
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_inputs, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()\n",
    "# When, we're happy with the outcome, we read test data from *test.csv* and predict labels for provided inputs.\n",
    "# \n",
    "# Test data contains only inputs and labels are missing. Otherwise, the structure is similar to training data.\n",
    "# \n",
    "# Predicted labels are stored into CSV file for future submission.\n",
    "# read test data from CSV file \n",
    "#test_inputs = pd.read_csv('generated_fuzzified_access_data_autoencoder.csv').values\n",
    "test_inputs = X_test\n",
    "test_inputs = test_inputs.astype(np.float)\n",
    "\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "#np.multiply(test_inputs, 1.0 / 255.0)\n",
    "\n",
    "# predict test set\n",
    "output_rows_train = predict.eval(feed_dict={x: train_inputs, keep_prob: 1.0})\n",
    "output_rows = predict.eval(feed_dict={x: test_inputs, keep_prob: 1.0})\n",
    "'''\n",
    "# using batches is more resource efficient\n",
    "predicted_lables = np.zeros(test_inputs.shape[0])\n",
    "for i in range(0,test_inputs.shape[0]//BATCH_SIZE):\n",
    "    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={x: test_inputs[i*BATCH_SIZE : (i+1)*BATCH_SIZE], keep_prob: 1.0})\n",
    "'''\n",
    "\n",
    "#np.savetxt('normalized_input_'+tag+'.csv', \n",
    " #          train_inputs,\n",
    "  #         delimiter=',', \n",
    "   #        header = headertext,\n",
    "    #       comments = '', \n",
    "     #      fmt='%f')\n",
    "\n",
    "# save results\n",
    "np.savetxt('autoencoded_input.csv', \n",
    "           output_rows, \n",
    "           delimiter=',', \n",
    "           header = headertext,\n",
    "           comments = '', \n",
    "           fmt='%f')\n",
    "\n",
    "# ## Appendix\n",
    "# It is good to output some variables for a better understanding of the process. \n",
    "# \n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "# Save the variables to disk.\n",
    "#save_path = saver.save(sess, \"./model_\"+tag+\".ckpt\")\n",
    "#print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "for w_,b_ in zip(W,b):\n",
    "    print(\"W:\")\n",
    "    print(w_.eval())\n",
    "    print(\"b:\")\n",
    "    print(b_.eval())\n",
    "\n",
    "sess.close()\n",
    "\n",
    "def main():\n",
    "    dataset = output_rows\n",
    "    # Get basic statistics of the loaded dataset\n",
    "    #dataset_statistics(dataset)\n",
    "\n",
    "    # Filter missing values\n",
    "    #dataset = handel_missing_values(dataset, HEADERS[6], '?')\n",
    "    #train_x, test_x, train_y, test_y = split_dataset(dataset, 0.67, HEADERS[0:-1], HEADERS[-1])\n",
    "\n",
    "    train_x = output_rows_train\n",
    "    test_x = output_rows\n",
    "    train_y = Y_train\n",
    "    test_y = Y_test\n",
    "    # Train and Test dataset size details\n",
    "    print(\"Train_x Shape :: \", train_x.shape)\n",
    "    print(\"Train_y Shape :: \", train_y.shape)\n",
    "    print(\"Test_x Shape :: \", test_x.shape)\n",
    "    print(\"Test_y Shape :: \", test_y.shape)\n",
    "\n",
    "    # Create random forest classifier instance\n",
    "    trained_model = random_forest_classifier(train_x, train_y)\n",
    "    print(\"Trained model :: \", trained_model)\n",
    "    predictions = trained_model.predict(test_x)\n",
    "    trained_answer = trained_model.predict(train_x)\n",
    "    #for i in xrange(0, 5):\n",
    "     #   print(\"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_y)[i], predictions[i]))\n",
    "\n",
    "    print(\"Train Accuracy :: \", accuracy_score(train_y, trained_answer))\n",
    "    print(\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\n",
    "    print(\" Confusion matrix \", confusion_matrix(test_y, predictions))\n",
    "    print(classification_report(test_y, predictions))\n",
    "    \n",
    "    \n",
    "    # Create logistic regression classifier instance\n",
    "    trained_model = logistic_regression_classifier(train_x, train_y)\n",
    "    print(\"Trained model :: \", trained_model)\n",
    "    predictions = trained_model.predict(test_x)\n",
    "    trained_answer = trained_model.predict(train_x)\n",
    "    #for i in xrange(0, 5):\n",
    "     #   print(\"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_y)[i], predictions[i]))\n",
    "\n",
    "    print(\"Train Accuracy :: \", accuracy_score(train_y, trained_answer))\n",
    "    print(\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\n",
    "    print(\" Confusion matrix \", confusion_matrix(test_y, predictions))\n",
    "    print(classification_report(test_y, predictions))\n",
    "\n",
    "    with open('ModelOutput.csv', 'w', newline='') as csv_write_file:\n",
    "        fieldnames = ['Autoencoder + RF']\n",
    "        csv_writer = csv.DictWriter(csv_write_file, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        index = 0\n",
    "        for i in train_y:\n",
    "            model_output.append(trained_answer[index])\n",
    "            index = index + 1\n",
    "        index = 0\n",
    "        for i in test_y:\n",
    "            model_output.append(predictions[index])\n",
    "            index = index + 1\n",
    "        index = 0\n",
    "        for i in model_output:\n",
    "            csv_writer.writerow({'Autoencoder + RF': model_output[index]})\n",
    "            index = index + 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
